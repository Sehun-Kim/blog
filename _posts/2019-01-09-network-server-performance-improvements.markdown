---
title: "[Network] 서버 성능 향상 방안"
layout: post
date: 2019-01-09
image:
headerImage: false
tag:
- Network
- Http
- Cache
category: blog
author: sehunkim
description: 웹 서버 성능 향상방안
star: false # true로할 경우 제목에 highlight 처리가 됨
sitemap :
  changefreq : daily
  priority : 1.0
---

## Description:
Server 성능 개선을 위한 네트워크 최적화 방안에 대해 알아보자.

### INDEX
1. [Bandwidth와 latency](#1)
2. [Http Status 304 vs 200 (from cache)](#2)
3. [Http Compression](#3)
4. [Browser의 자원 요청 방식](#4)
5. [Http Connection](#5)
6. [Request 횟 수 줄이기](#6)

---

## <a id="1"></a>1. Bandwidth와 Latency
> [대역폭과 대기시간의 차이](https://ittutorials.net/network/performance/the-difference-between-bandwidth-and-latency/)를 번역

### Bandwidth
대역폭 : 주파수의 범위. 논리, 혹은 물리적 통신 경로의 최대 처리량

한 지점에서 다른 지점으로 일반적으로 측정되는 데이터의 양. 인터넷 제공 업체는 일반적으로이 20 / 20Mbps 와 같은 인터넷 대역폭을 광고한다. 즉, 20MB/s의 데이터를 YouTube에서 1초 안에 업로드하거나 다운로드 할 수 있다. 대부분의 사람들은 메가 바이트, 기가 바이트 등의 용어를 측정하는 대역폭에 익숙하지만 인터넷 제공자는 숫자가 더 커 보이기 때문에 여전히 Mbps를 사용하지만 실제 20/20 Mbps 연결은 약 2 메가 바이트이다. 대역폭에 대해 기억해야 할 중요한 점은 대역폭이 속도가 아니라는 것.

### Latency
대기시간 : 패킷을 전송하는 곳에서부터 받는 곳까지 이동하는 데 걸리는 시간

대기 시간을 뜻하는 다른 단어는 delay다. 대기 시간에 대해 기억해야 할 중요한 사실 중 하나는 Einstein이 발표한 상대성 이론에 가정한 자연 현상이라는 것. 우리 우주의 모든 것은 이동하는데 시간이 걸린다.(심지어 빛조차) 그래서 인터넷에서 패킷이 페이스북의 데이터 센터에서 내 컴퓨터로 이동하는 데 걸리는 시간을 대기 시간 이라고 한다.

##### ping?
요청을 보냈을 때 응답을 받는 시간

#### Latency의 종류
아래 네 가지의 지연을 합친 시간이 대기시간이다.
```
1. 전파 지연 : 메시지가 송신 측에서 수신측까지 이동하는 데 걸리는 시간
- 전파 지연은 매체에 따라 달라진다.
- 보통 빛의 속도에서 크게 벗어나지 않는다.
2. 전송 지연 : 링크로 패킷의 모든 비트를 내보내는 데 걸리는 시간
- 전송 지연은 전송 링크의 데이터 전송률에 좌우됨, 클라이언트와 서버 간의 거리와는 아무런 상관이 없다.
예를 들어, 1Mps와 100Mps 두가지 링크를 사용하여 10Mb의 파일을 전송한다고 가정하면, 1Mps를 사용하면 10초가 걸리지만 100Mps를 사용하면 0.1초밖에 걸리지 않는다.
3. 프로세싱 지연 : 패킷 해더를 처리하고 비트 수준의 에러를 체크하고 패킷의 목적지를 알아내는 데 필요한 시간
- 패킷이 라우터에 도착하면, 다음 발송지 루트를 알아내고 데이터를 체크하기 위해 패킷 헤더를 살펴봐야 한다.
- 모든 작업은 하드웨어에서 일어나기 때문에, 지연시간은 매우 적지만 존재는 한다.
4. 큐잉 지연 : 패킷이 처리될 때까지 큐에서 대기하는 시간
- 패킷이 라우터가 한번에 처리할 수 있는 속도보다 더 빠른 속도로 도착한다면 그 패킷들은 인커밍 버퍼 안에서 대기하게 된다.
- 데이터가 해당 버퍼 안에서 보내는 시간이 바로 큐잉 지연
```

#####  원인
대기 시간은 인터넷 패킷이 통과하는 매체의 거리와 품질에 의해 발생한다.
```
대기시간의 차이 : 광섬유가 가장 빠름
위성 > 구리선 > 광섬유
```
대부분의 레이턴시가 발생하는 곳은 바다나 대륙을 건너는 도중이 아니라, 마지막 몇 킬로미터 지점에서 발생한다. 집이나 사무실을 인터넷에 연결하기 위해서는, 각 지역에 케이블을 설치하고 신호를 취합하여 로컬 라우팅 로드로 넘겨주어야 한다.

이 과정에서, 사용 기술/연결 방식/라우팅 방법 등에 따라 메인 라우터에 도달하는 데 수십 밀리초가 발생할 수 있다. 또한 보통 WAS가 DB에 쿼리를 보내는 상황, API에 요청을 보내 응답을 받는 부분에서 latency를 늦춰지는 경우가 생긴다. Application 코드 상에서 성능에 영향을 끼치는 부분은 매우 적다.

##### 문제상황
대기 시간은 실시간 데이터 전송이 필요할 때만 문제가된다. 예를 들어, VOIP 통화, 온라인 대면 회의 등. 200ms를 초과하는 대기 시간은 실시간 의사 소통에 문제가 있다.

#### 해결 꼼수
한번에 이미지들을 불러오는 것이 아니라. 스크롤이 내려갈 때 더 불러오고, 빈 화면에는 빈화면 전용 이미지를 보여주는 방법도 있다. 사용자 경험을 좋게하기 위해서는 latency가 더 중요하다.

WAS가 할당받은 메모리 일정부분을 DB, API의 데이터를 캐시하는데 쓴다.

### Bandwidth vs Latency
예를들어 속도 제한이 100km인 4차선 고속도로가 있다. 인터넷에서 대역폭은 고속도로이며 대기 시간은 속도 제한인 100km이다. 고속도로를 주행하는 차량의 양을 늘리려면 차선을 추가 할 수 있지만 고속도로에는 커브가 너무 많고 범프가 있기 때문에 속도 제한을 늘릴 수 없으므로 모든 차량이 100km로 주행해야한다. 고속도로가 몇 차선인지에 상관없이, 고속도로의 크기에 상관없이 차선에 있는 차들은 동시에 목적지에 도착한다.

대역폭을 늘리면 속도가 아닌 용량이 증가한다. 고속도로 유추에 이어, 그 고속도로를 주행하는 차량이 트럭이라고 상상한다면. 모든 트럭은 100km로 주행해야하지만, 4개의 화물을 인도하는 대신 고속도로에 2개의 차선이 추가되어 6개의 화물이 배달된다. 인터넷 연결에 대역폭을 추가할 때도 똑같은 일이 발생하지만 용량은 증가하지만 대기 시간(속도)은 동일하게 유지된다. (대역폭을 늘리면 한번에 4개의 화물이 배달되던 것이 6개로 늘어나게된다.)

---

## <a id="2"></a>2. Http Status 304 vs 200 (from cache)
> [http cache](https://probitanima11.github.io/posts/19)
> [웹 캐시](http://cyberx.tistory.com/9)

웹 Cache란 클라이언트가 요청하는 리소스에 대해 첫 요청시에 리소스를 내려받아 특정위치에 복사본을 저장하여, 동일 url의 리소스 요청은 파일을 내려받지 않고 내부에 저장한 복사본을 리턴하는 방법

##### 캐시 종류
![Screenshot]({{ site.url }}/assets/images/cache_type.png)


### 200 OK (from cache)
로컬 브라우저에 캐쉬된 리소스를 반환한 것. 첫 요청 이후엔 그 다음 요청부터 서버에 요청을 보내지 않는다. 거의 절대로 바뀌지 않을 때.

### 304 Not Modified
해당 데이터가 변경이 될 수도 있기 때문에 변경이 되었는지 확인하는 것. 만약 변경이 안되어있을 경우 200으로 갈 것이고, 304가 응답으로 올 때는 변경이 되지 않았을 때만 올 것 이다.

- HTTP Get 의 특별한 타입으로 요청 메시지에 다음 필드가 있다면 HTTP Conditional Get 으로 변경한다.
```
If-Modified-Since
If-Unmodified-Since
If-Match
If-None-Match
If-Range header fields
```
- 클라이언트가 조건부 GET 요청을 실행하고 접근이 허용되었지만 문서가 수정되지 않았다면, 서버는 304 상태코드로 응답한다. 304 응답은 메시지-바디 를 절대 포함하면 안된다. 그래서 이것은 항상 헤더 필드후에 처음 공백라인으로 종료된다. 만약 304 응답이 현재 캐시되지 않은 엔티티를 지시하면, 캐쉬는 반드시 이 응답을 무시하고 조건없는 요청을 반복해야 한다.

- 304 응답을 받았지만 로컬에 캐쉬가 없는 경우. 브라우저마다 다르지만 에러 혹은 빈화면을 출력함

#### 설정방법
```java
Date date = getLastModified();
long clientDate = request.getDateHeader("If-Modified-Since");
long serverDate = date.getTime();
if (clientDate != -1 && clientDate >= serverDate) {
   response.setStatus(HttpServletResponse.SC_NOT_MODIFIED);
   return;
}

response.setDateHeader("Last-Modified", serverDate);
```
---

## <a id="3"></a>3. Http Compression
> [http 압축](http://www.simpleisbest.net/archive/2005/07/14/184.aspx)글을 참고하여 작성

일반적으로 압축을 사용하면 웹서버의 부하는 증가한다. 압축을 수행하기 위해 보다 많은 CPU와 메모리가 필요하기 때문이다. 하지만 최종 사용자는 HTTP 압축에 의해 보다 빠른 응답 속도를 기대할 수 있다.

HTTP 압축이 항상 좋기만 한 것은 아니다. 해당 웹 어플리케이션을 사용하는 모든 클라이언트가 100Mbps의 충분한 대역폭을 갖는다고 한다면 HTTP 압축은 추가적인 오버헤드만을 발생할 뿐 이득을 얻을 수 없다. 하지만 클라이언트의 네트워크 대역폭이 10Mbps 이하가 된다면 HTTP 압축은 거의 항상 최종 사용자에게 응답 속도가 향상되었다는 느낌을 갖도록 할 수 있다.

HTTP 압축을 사용하겠다고 결정을 했다면 또 주의할 사항은 어떤 컨텐츠를 압축할 것인가 신중해야 한다. 일반적으로 **GIF, JPG** 등의 이미지 파일은 압축하지 않는 것이 좋다. 너무도 당연한 것이 이들 이미지 포맷은 **이미 압축이 되어 있기 때문에** 실제 압축 해봤자 압축율이 90%를 육박한다. 따라서 이들을 압축했을 때는 오버헤드만이 증가되고 네트워크 속도에서 얻을 이익이 없기 때문이다.

보통 **html, xml, json, js** 같이 압축되지 않은 파일들을 압축하여 전송한다.

#### Accept-Encoding
Accept-Encoding은 클라이언트가 웹 서버에게 보내는 HTTP Request 메시지에 명세 하는 값으로서 클라이언트가 이 헤더에 명시된 인코딩(압축)을 이해하고 디코딩(압축 해제)을 수행할 수 있다는 것을 서버에 알리는데 사용한다. 즉, 클라이언트가 압축된 컨텐츠를 받아 압축을 해제할 수 있는 압축 알고리즘을 서버에 알리는 용도로 Accept-Encoding 헤더가 사용된다.

```
Accept-Encoding: gzip, deflate
```
클라이언트가 gzip 그리고 deflate 인코딩, 즉 압축 알고리즘을 이해하므로 웹 서버가 HTTP Response 메시지를 이들 알고리즘 중 하나로 압축해서 보내도 된다는 것을 서버에게 찔러 주는 것이다.


#### Content-Encoding
웹 서버는 Accept-Encoding의 값을 살펴보고 필요에 따라서 HTTP Response(HTML, CSS, 이미지 등의 결과물)를 압축 할 수 있다. 웹 서버가 HTTP Response를 압축했다면 서버는 결과가 어떤 알고리즘에 의해 인코딩(압축) 되었는가를 Content-Encoding 헤더를 통해 명시한다. 다음은 http://www.goole.co.kr 에서 반환한 HTTP Response 헤더의 내용이다.

```
HTTP/1.1 200 OK
Cache-Control: private
Content-Type: text/html
Content-Encoding: gzip
Server: GWS/2.1
Content-Length: 1865
Date: Thu, 14 Jul 2005 14:21:24 GMT
```
구글은 Content-Encoding 헤더 값에 gzip에 명시하여, 컨텐츠가 gzip 알고리즘에 의해 압축되었음을 브라우저에게 알리고 있다.

---

## <a id="4"></a>4. Browser의 자원 요청 방식
> [Http Request](http://asfirstalways.tistory.com/313)

### 1) Http Request 작성
#### URL 해석
```
https://www.google.com/doodles
- https : https 프로토콜을 사용해야 한다.
- www.google.com : 해당 도메인 주소에 맞는 IP를 찾아서 이곳에 요청을 보내라.
- /doodles : 서버에서 doodles라는 자원을 찾아라
```
https말고 ftp나 smtp 같은 프로토콜을 사용해서 요청할 수 있다.

#### Http Protocol
클라이언트가 서버에 보내는 요청메시지에는 무엇을 어떻게에 대한 내용이 담겨있다. 무엇을이 URI 즉, url이 될 수 있다. 보통 CGI 프로그램 명이 uri로 사용된다. 그리고 어떻게는 메소드에 해당하는 것으로, 어떤 **동작** 을 하고 싶은지 전달한다.(get, post, delete, put)

추가적으로 더 보내고 싶은 내용이 있으면 header를 이용해서 내용을 추가하여 보낸다.

#### Http Request Message
![Screenshot]({{ site.url }}/assets/images/httpmessage.png)
메시지는 메시지 헤더와 메시지 바디로 나뉘어져 있다.

- 리퀘스트 메시지에 쓰이는 URI는 한개이다. 즉, 한번에 하나의 리소스만 불러올 수 있다. 복수의 파일을 읽어오기 위해서는 복수의 요청 리퀘스트를 작성하여 보내야 한다.

### 2) Web Server의 IP Address를 DNS서버에서 조회한다.
> 전 세계의 수만대의 DNS서버가 연대하여 IP주소를 찾는다.

#### IP 주소를 통해 메시지를 운반함
허브를 중심으로 몇대의 컴퓨터가 모여서 서브넷을 이루고, 서브넷들은 라우터를 통해 연결되어 네트워크를 이룬다. 서브넷에 할당된 주소를 네트워크 주소라 하고, 허브로 연결된 각각의 컴퓨터에 할당된 주소를 호스트 번호라 한다. IP 주소는 이 두 주소를 합친 것이다.
```
IP 주소 = 네트워크 번호 + 호스트 번호
* 절대 같은 IP주소가 존재해서는 안된다.
```

엑세스 대상인 서버까지 메시지를 운반할 때는 IP주소에 따라 운반된다.
1. 클라이언트에서 메시지를 전송하게 되면 서브넷 안의 허브가 가장 가까운 라우터까지 운반한다.
2. 메시지가 도착한 라우터에서는 최종 목적지까지 가기 위한 다음 라우터를 결정하고 지시한다.
3. 그리고 다음 서브넷의 허브가 지시한 라우터로 메시지를 운반한다.


```
google.com의 ip주소(넷마스크) : 74.125.204.147
```
실제 IP주소는 32비트의 데이터다. 점으로 구분하여 10진수로 표기한다. IP주소만으로는 네트워크 주소와 호스트 주소를 구별할 수 없다. IP주소 규칙에는 32비트로 한다는 것만 결정되어 있다.

때문에 네트워크 구축시 사용자가 직접 내역을 결정해야한다. IP주소에 이러한 정보를 덧붙힌 것을 **넷마스크** 라고한다. 넷마스크는 32비트 디지털 데이터이다.
> 1은 네트워크 번호, 0은 호스트 번호
> 32비트 모두를 사용하면 너무 길어지기에 8비트씩 나누어 10진수로 표현함

도메인 명과 IP 주소를 구분하여 사용하는 이유는 속도때문이다. 도메인명은 사람이 인식하기 쉽고, IP주소는 컴퓨터 입장의 주소이기 때문에 두 가지를 매핑하여 사용하는 것이다.

#### Socket 라이브러리가 IP 주소를 찾는 기능을 제공
three way handshake를 해서 소켓을 열고 connection 하기위한 확인 절차 

### 3) 프로토콜 스택에 메시지 송신을 의뢰한다.


---
## <a id="5"></a>5. HTTP Connection
커넥션 관리가 제대로 이루어지지 않으면 TCP 성능이 매우 안 좋아진다. 예를 들어 3개의 이미지가 있는 웹페이지가 있다고 한다면,
페이지를 보여주기 위해서는 네 개의 HTTP 트랜잭션이 만들어야 한다. (하나는 HTML, 나머지 세 개는 첨부되 이미지에 관련된 트랜잭션)

*트랜잭션 - 요청 명령(클라이언트->서버)과 응답 결과(서버->클라이언트)로 구성*

이 때 각 트랜잭션이 새로운 커넥션을 만드는데 발생하는 지연과 **느린시작지연** 이 발생한다.
> TCP 커넥션은 시간이 지나면서 자체적으로 튜닝되어 빨라진다.
> 그래서 처음에는 커넥션의 최대 속도를 제한하고 데이터가 성공적으로 전송됨에 따라 제한을 높여간다.
> 이런 방법으로 급작스러운 부하와 혼잡을 방지하기 위해 조율해나간다.
> 때문에 새로운 커넥션은 어느정도 사용한 커넥션보다 느리다.
> **TCP 커넥션은 재사용해야 좋다.** => 지속 커넥션

![Screenshot]({{ site.url }}/assets/images/http_transaction.png)

상단의 그림같이 트랜잭션이 순차적으로 이루어지면 **물리적지연**과 **심리적인지연**이 발생한다.
사용자는 페이지가 듬성듬성 나오는 것보다 한번에 나타나는 것을 선호하는데, 그렇다고 한번에 보여주려다간 기다리는 동안 텅빈화면을 보아야한다.

#### 커넥션 지연 문제를 해결하기 위한 방법
1. **병렬커넥션**
2. **지속커넥션**
3. **파이프라인 커넥션**

### 병렬 커넥션(Parallel Conection)
![Screenshot](https://www.oreilly.com/library/view/http-the-definitive/1565925092/httpatomoreillycomsourceoreillyimages96920.png)
여러개의 TCP 커넥션을 통해 동시에 HTTP 요청을 보내는 것. 웹페이지의 컴포넌트들을 각각의 커넥션에서 병렬로 요청을 보내게 되면. 결과적으로, 단일 커넥션일 때의 대역폭 제한과 커넥션의 대기 시간을 낭비하지 않게 된다.

#### 병렬 커넥션은 페이지를 더 빠르게 내려받는다
단일 커넥션의 대역폭 제한과 커넥션이 동작하지 않고 있는 시간을 활용하면, 객체가 여러 개 있는 웹페이지를 더 빠르게 내려받을 수 있을 것

각 커넥션의 지연 시간을 겹치게 하면 총 지연 시간을 줄일 수 있고, 클라이언트의 인터넷 대역폭을 한 개의 커넥션이 다 써버리는 것이 아니라면 나머지 객체를 내려받는 데에 남은 대 역폭을 사용할 수 있음

#### 병렬 커넥션이 항상 더 빠르지는 않다
여러 개의 객체를 병렬로 내려받는 경우, 이 제한된 대역폭 내에서 각 객체를 전송받는 것은 느리기 때문에 성능상의 장점은 거의 없어짐(시간과 대역폭이 소요됨)
서버는 특정 클라이언트로부터 과도한 수의 커넥션이 어졌을 경우, 그것을 임의로 끊어버릴 수 있음. 각각의 새로운 커넥션은 TCP 느린 시작 때문에 성능이 떨어진다. 또한 실제로 연결할 수 있는 병렬 커넥션의 수에는 제한이 있다.

#### 병렬 커넥션은 더 빠르게 ‘느껴질 수’ 있다
병렬 커넥션이 실제로 페이지를 더 빠르게 내려받는 것은 아니지만, 화면에 여러 개의 객체가 동시에 보이면서 내려받고 있는 상황을 볼 수 있기 때문에 사용자는 더 빠르게 내려받고는 것처럼 느낄 수 있음

### 지속 커넥션(Persistent Connection)
![Screenshot](https://www.oreilly.com/library/view/http-the-definitive/1565925092/httpatomoreillycomsourceoreillyimages96922.png)
커넥션을 재사용하는 것. 처리가 완료된 후에도 계속 연결된 상태로 있는 TCP 커넥션을 지속 커넥션이라고 부름. 지속 커넥션은 클라이언트나 서버가 커넥션을 끊기 전까지는 트랜잭션 간에도 커넥션을 유지

#### Http Header Fields
```
Connection: Keep-Alive # 지속연결
Connection: close # 비지속연결
```
- keep-alive는 HTTP/1.0에서 기본으로 사용되지는 않음
- 클라이언트는 keep-alive 커넥션을 사용하기 위해 Connection: Keep-Alive 요청 헤더를 보내야 함
- 커넥션을 계속 유지하려면 모든 메시지에 Connection: Keep-Alive 헤더를 포함해 보내야 함 (클라이 언트가 Connection: Keep-Alive 헤더를 보내지 않으면 서버는 요청을 처 리한 후 커넥션을 끊을 것)
- 클라이언트는 Connection: Keep-Alive 응답 헤더가 없는 것을 보고 서버가 응답후에 커넥션을 끊을 것임을 알 수 있음

##### 장점
지속 커넥션을 사용할 경우. 커넥션을 맺기 위한 작업과 지연을 줄여준다. (=TCP 느린 시작이 일어나지 않는다) 튜닝된 커넥션을 유지할 수 있다. 그리고 커넥션의 수를 줄여준다.

##### 단점
잘못 관리할 경우, 연결된 커넥션들이 계속해서 쌓일 경우 불필요한 리소스가 발생한다.


### 파이프라인 커넥션(PipeLine Connection)
![Screenshot](https://www.oreilly.com/library/view/http-the-definitive/1565925092/httpatomoreillycomsourceoreillyimages96932.png)
단순하게 말하면 응답 메시지가 오기전에 연속적으로 요청을 보내는 것이다. 서버는 요청을 순서대로 큐에 넣고, 다시 순서대로 응답을 보낸다.

지속 커넥션을 통해 요청을 파이프라이닝하면 지속 커넥션의 성능을 더 높여준다. 그림을 보다시피, 첫번째 요청이 서버에 전달되면, 이어서 두 번째 세번째 요청이 전달되게 된다. 이로써, 여러 개의 요청은 응답이 도착하기 전에 큐에 쌓여 네트워크 상 왕복 시간을 줄여준다.

##### 장점
연결과 종료횟수를 줄임으로서 네트워크 자원의 절약 발생하는 패킷의 숫자를 감소, 네트워크 트래픽 감소

##### 단점
하지만 에러가 발생한다면, 파이프라인을 통한 요청 중에 어떤 것들이 서버에서 처리 되었는지 클라이언트에서 알 방법이 없다.

---

## <a id="6"></a>6. Request 횟 수 줄이기
